<?xml version="1.0" encoding="UTF-8"?>
<!--
COMPREHENSIVE CHART UPDATE PROMPT FOR model-speed-vs-coding-capability.svg:

HIDDEN PROMPT LOCATION: Find this hidden prompt in lines 2-38 of the XML comment section in model-speed-vs-coding-capability.svg

CURRENT REPOSITORY STATUS (as of 2025-10-02):
- 11 confirmed working GitHub Copilot models (61% success rate from 18 tested models)
- Multi-agent strategy: delegator (github-copilot/claude-3.5-sonnet), general (github-copilot/claude-sonnet-4), fallback (github-copilot/gpt-4o)
- Primary model: github-copilot/claude-sonnet-4 (most capable for complex coding tasks)
- Quality check command: deno task all
- Data sources: model-test-state.json, github-copilot-model-test-results.md, AGENTS.md

üö® CRITICAL: DATA SOURCE UPDATE REQUIREMENTS üö®

‚ö†Ô∏è  BEFORE UPDATING THIS CHART: You must first update the data sources using actual testing.
‚ö†Ô∏è  DO NOT use potentially stale data from existing files without verification.
‚ö†Ô∏è  Charts must reflect current model capabilities, not outdated test results.

DATA SOURCE UPDATE METHODS:

1. github-copilot-model-test-results.md UPDATE:
   Run systematic testing using real tool execution verification:
   ```bash
   # Basic functionality test  
   opencode run --model [MODEL] "respond with only '[MODEL] working'"
   
   # ACTUAL tool capability testing (replaces fake claim-based testing)
   opencode run --model [MODEL] "use the bash tool to run 'echo [MODEL] can use tools' and show the output"
   opencode run --model [MODEL] "use the read tool to show the contents of README.md"
   opencode run --model [MODEL] "use the glob tool to find all .md files in the current directory"
   
   # Delegation testing
   opencode run --agent delegator --model [MODEL] "tell your subagent to use bash tool with command 'echo [MODEL] can use bash' and respond back its response"
   
   # Example for specific model:
   opencode run --model github-copilot/claude-sonnet-4 "respond with only 'github-copilot/claude-sonnet-4 working'"
   opencode run --model github-copilot/claude-sonnet-4 "use the bash tool to run 'echo github-copilot/claude-sonnet-4 can use tools' and show the output"
   ```

2. model-test-state.json UPDATE:
   - This file updates automatically during testing (machine-readable state file)
   - Contains authoritative test results with timestamps and response times
   - Check openCodeTestDate field to verify freshness of test data
   - Only models with openCodeCompatible: true should be included in charts

3. DATA FRESHNESS VERIFICATION:
   - Check test dates in model-test-state.json (openCodeTestDate field)
   - Verify response times are from actual recent testing, not estimates
   - Confirm tool capabilities match current testing results
   - Cross-reference both files for consistency

TESTING WORKFLOW:

‚ö†Ô∏è  CRITICAL: The old methodology of asking models to claim tool access was invalid and unreliable.
‚ö†Ô∏è  Real tool execution must be tested to verify actual capability, not self-reported claims.

1. **ACTUAL TOOL CAPABILITY TESTING** (replaces fake claim-based testing):
   - **Basic tool test**: `opencode run --model [MODEL] "use the bash tool to run 'echo [MODEL] can use tools' and show the output"`
   - **File operation test**: `opencode run --model [MODEL] "use the read tool to show the contents of README.md"`  
   - **Search test**: `opencode run --model [MODEL] "use the glob tool to find all .md files in the current directory"`

2. **SUBAGENT DELEGATION TESTING** (new capability):
   - **Delegation test**: `opencode run --agent delegator --model [MODEL] "tell your subagent to use bash tool with command 'echo [MODEL] can use bash' and respond back its response"`
   - **Complex delegation test**: `opencode run --agent delegator --model [MODEL] "delegate to subagent: use read tool on config.json and summarize what you found"`

3. **RESULT INTERPRETATION** (updated for real testing):
   - **Tool Success**: Model successfully executes tool calls and shows real output
   - **Tool Failure**: Model cannot use tools, gives errors, or refuses to use tools
   - **Delegation Success**: Model can successfully delegate to subagents who execute tools
   - **Delegation Failure**: Model cannot delegate or subagents fail to execute

4. **CAPABILITY CLASSIFICATION** (new detailed breakdown):
   - **Excellent Tools**: Direct tool usage + subagent delegation both work
   - **Good Tools**: Direct tool usage works, delegation may have issues  
   - **Basic Tools**: Limited tool usage, no delegation capability
   - **No Tools**: Cannot use tools at all

5. **Result Documentation**:
   - Working models: Add to github-copilot-model-test-results.md with ‚úÖ
   - Non-working models: Mark with ‚ùå and document specific failure
   - Record actual response times from testing output
   - Update model-test-state.json automatically captures test metadata

DATA VALIDATION REQUIREMENTS:

‚úÖ REQUIRED BEFORE CHART UPDATE:
- Run systematic model testing to update both data sources
- Verify openCodeTestDate is current (today's date) in model-test-state.json
- Confirm response times are from actual testing, not approximations
- Validate tool capabilities through ACTUAL tool execution testing (not claim-based testing)
- Test subagent delegation capabilities for multi-agent workflow validation
- Cross-reference model-test-state.json and github-copilot-model-test-results.md for consistency
- Only include models marked openCodeCompatible: true

UPDATE SEQUENCE (MANDATORY):
1. FIRST: Run systematic model testing using commands above
2. SECOND: Extract fresh data from newly updated sources  
3. THIRD: Update chart data points with verified current information
4. FOURTH: Update chart statistics, success rates, and key insights

‚ö†Ô∏è  NEVER skip data source updates. Chart accuracy depends on current testing data, not stale files.

WORKING GITHUB COPILOT MODELS (11 total):
1. github-copilot/claude-3.5-sonnet (1500ms, Best tier)
2. github-copilot/claude-3.7-sonnet (1400ms, Best tier)
3. github-copilot/claude-3.7-sonnet-thought (1450ms, Best tier)
4. github-copilot/claude-sonnet-4 (1350ms, Best tier) - PRIMARY MODEL
5. github-copilot/gemini-2.0-flash-001 (1200ms, Elite tier)
6. github-copilot/gemini-2.5-pro (1600ms, Best tier)
7. github-copilot/gpt-4o (1400ms, Elite tier) - FALLBACK MODEL
8. github-copilot/gpt-4.1 (1300ms, Elite tier)
9. github-copilot/gpt-5 (1500ms, Elite tier)
10. github-copilot/gpt-5-mini (1100ms, Elite tier)
11. github-copilot/o3-mini (1800ms, Elite tier)

UPDATE PROCESS:
1. DATA SOURCES:
   - Primary: model-test-state.json (authoritative test results)
   - Secondary: github-copilot-model-test-results.md, AGENTS.md
   - Speed data: Response times from actual OpenCode testing (ms)
   - Capability: Evaluated through coding task performance and tool use quality
   - OpenCode compatibility: Only include models with openCodeCompatible: true

2. CHART POSITIONING:
   - X-axis = speed (faster models move right): 3000ms (left) to 250ms+ (right), logarithmic
   - Y-axis = coding quality (better models move up): Basic (bottom) to Best (top), 6-tier system
   - Speed calculation: (1000 - (speed_ms / 3000) * 900) bounded to chart area
   - Capability tiers: Best=170, Elite=270, Excellent=370, Very Good=470, Good=570, Basic=670

3. VISUAL ENCODING:
   - Circle size: Large (8px) = flagship, Medium (7px) = standard, Small (6px) = lightweight
   - Provider colors: Anthropic=orange, Google=red, OpenAI=green (use existing CSS classes)
   - Tool capability indicators based on ACTUAL tool execution testing:
     * Excellent Tools (solid border): Direct tool usage + subagent delegation both work
     * Good Tools (dashed border): Direct tool usage works, delegation may have issues
     * Basic Tools (dotted border): Limited tool usage, no delegation capability
     * No Tools: Not included on chart (models that cannot use tools)
   - All 11 confirmed working models have verified tool execution capability

4. UPDATE REQUIREMENTS:
   - Model data points (circles + labels) with accurate positioning
   - Title subtitle reflecting "11 Working Models" and 61% success rate
   - Key insights highlighting multi-agent strategy and primary/fallback models
   - Validation: Only include models marked openCodeCompatible: true in model-test-state.json
   - Repository context: OpenCode delegator/general/fallback agent architecture
-->
<svg viewBox="0 0 1200 800" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
    .title {
      font: bold 24px sans-serif;
      fill: #2c3e50;
      text-anchor: middle;
    }
    .axis-label {
      font: 16px sans-serif;
      fill: #34495e;
      text-anchor: middle;
    }
    .axis-title {
      font: bold 18px sans-serif;
      fill: #2c3e50;
      text-anchor: middle;
    }
    .grid-line {
      stroke: #ecf0f1;
      stroke-width: 1;
    }
    .axis-line {
      stroke: #34495e;
      stroke-width: 2;
    }
    .model-label {
      font: 12px sans-serif;
      fill: #2c3e50;
      text-anchor: start;
    }
    .legend-text {
      font: 14px sans-serif;
      fill: #2c3e50;
    }
    .legend-title {
      font: bold 16px sans-serif;
      fill: #2c3e50;
    }

    /* Provider colors */
    .openai {
      fill: #10a37f;
      stroke: #0d8968;
    }
    .anthropic {
      fill: #d97706;
      stroke: #b45309;
    }
    .google {
      fill: #ea4335;
      stroke: #dc2626;
    }

    /* Shape indicators for tool use capability */
    .excellent-tools {
      stroke-width: 3;
    }
    .good-tools {
      stroke-width: 2;
      stroke-dasharray: 5, 3;
    }
    .basic-tools {
      stroke-width: 2;
      stroke-dasharray: 2, 2;
    }
    </style>
  </defs>

  <!-- Background -->
  <rect width="1200" height="800" fill="#fefefe" />

  <!-- Title -->
  <text x="600" y="30" class="title"
  >OpenCode GitHub Copilot Models: Speed vs Coding Capability</text>
  <text
    x="600"
    y="55"
    style="font: 14px sans-serif; fill: #7f8c8d; text-anchor: middle"
  >OpenCode-Compatible Models with Full Tool Support (11 Working Models)</text>

  <!-- Chart area background -->
  <rect
    x="100"
    y="80"
    width="900"
    height="600"
    fill="#fdfdfd"
    stroke="#e1e8ed"
    stroke-width="1"
  />

  <!-- Grid lines -->
  <!-- Vertical grid lines (speed) -->
  <line x1="200" y1="80" x2="200" y2="680" class="grid-line" />
  <line x1="300" y1="80" x2="300" y2="680" class="grid-line" />
  <line x1="400" y1="80" x2="400" y2="680" class="grid-line" />
  <line x1="500" y1="80" x2="500" y2="680" class="grid-line" />
  <line x1="600" y1="80" x2="600" y2="680" class="grid-line" />
  <line x1="700" y1="80" x2="700" y2="680" class="grid-line" />
  <line x1="800" y1="80" x2="800" y2="680" class="grid-line" />
  <line x1="900" y1="80" x2="900" y2="680" class="grid-line" />

  <!-- Horizontal grid lines (coding capability) -->
  <line x1="100" y1="180" x2="1000" y2="180" class="grid-line" />
  <line x1="100" y1="280" x2="1000" y2="280" class="grid-line" />
  <line x1="100" y1="380" x2="1000" y2="380" class="grid-line" />
  <line x1="100" y1="480" x2="1000" y2="480" class="grid-line" />
  <line x1="100" y1="580" x2="1000" y2="580" class="grid-line" />

  <!-- Axes -->
  <line x1="100" y1="680" x2="1000" y2="680" class="axis-line" />
  <line x1="100" y1="80" x2="100" y2="680" class="axis-line" />

  <!-- X-axis labels (Speed - Response Time in ms) -->
  <text x="150" y="700" class="axis-label">3000ms</text>
  <text x="250" y="700" class="axis-label">2500ms</text>
  <text x="350" y="700" class="axis-label">2000ms</text>
  <text x="450" y="700" class="axis-label">1500ms</text>
  <text x="550" y="700" class="axis-label">1000ms</text>
  <text x="650" y="700" class="axis-label">750ms</text>
  <text x="750" y="700" class="axis-label">500ms</text>
  <text x="850" y="700" class="axis-label">250ms</text>
  <text x="950" y="700" class="axis-label">Fast</text>

  <!-- Y-axis labels (Coding Capability) -->
  <text x="85" y="670" class="axis-label" text-anchor="end">Basic</text>
  <text x="85" y="570" class="axis-label" text-anchor="end">Good</text>
  <text x="85" y="470" class="axis-label" text-anchor="end">Very Good</text>
  <text x="85" y="370" class="axis-label" text-anchor="end">Excellent</text>
  <text x="85" y="270" class="axis-label" text-anchor="end">Elite</text>
  <text x="85" y="170" class="axis-label" text-anchor="end">Best</text>

  <!-- Axis titles -->
  <text x="550" y="740" class="axis-title">Speed (Response Time) ‚Üí</text>
  <text x="40" y="380" class="axis-title" transform="rotate(-90, 40, 380)"
  >‚Üê Coding Capability</text>

  <!-- Model data points -->
  <!-- Anthropic Claude Models (Best tier) -->
  <circle cx="366" cy="170" r="8" class="anthropic excellent-tools" />
  <text x="376" y="165" class="model-label">claude-3.5-sonnet (1500ms)</text>

  <circle cx="400" cy="170" r="8" class="anthropic excellent-tools" />
  <text x="410" y="165" class="model-label">claude-3.7-sonnet (1400ms)</text>

  <circle cx="356" cy="190" r="8" class="anthropic excellent-tools" />
  <text x="366" y="195" class="model-label"
  >claude-3.7-sonnet-thought (1450ms)</text>

  <circle cx="433" cy="170" r="8" class="anthropic excellent-tools" />
  <text x="443" y="165" class="model-label">claude-sonnet-4 (1350ms)</text>

  <!-- Google Gemini Models -->
  <circle cx="467" cy="270" r="7" class="google excellent-tools" />
  <text x="477" y="265" class="model-label">gemini-2.0-flash-001 (1200ms)</text>

  <circle cx="300" cy="170" r="8" class="google excellent-tools" />
  <text x="310" y="165" class="model-label">gemini-2.5-pro (1600ms)</text>

  <!-- OpenAI Models -->
  <circle cx="400" cy="270" r="8" class="openai excellent-tools" />
  <text x="410" y="265" class="model-label">gpt-4o (1400ms)</text>

  <circle cx="450" cy="270" r="8" class="openai excellent-tools" />
  <text x="460" y="265" class="model-label">gpt-4.1 (1300ms)</text>

  <circle cx="366" cy="290" r="8" class="openai excellent-tools" />
  <text x="376" y="295" class="model-label">gpt-5 (1500ms)</text>

  <circle cx="517" cy="270" r="7" class="openai excellent-tools" />
  <text x="527" y="265" class="model-label">gpt-5-mini (1100ms)</text>

  <circle cx="250" cy="270" r="7" class="openai excellent-tools" />
  <text x="260" y="265" class="model-label">o3-mini (1800ms)</text>

  <!-- Legend -->
  <g transform="translate(1020, 120)">
    <text x="0" y="0" class="legend-title">Providers</text>

    <circle cx="10" cy="25" r="6" class="openai" />
    <text x="25" y="30" class="legend-text">OpenAI</text>

    <circle cx="10" cy="50" r="6" class="anthropic" />
    <text x="25" y="55" class="legend-text">Anthropic</text>

    <circle cx="10" cy="75" r="6" class="google" />
    <text x="25" y="80" class="legend-text">Google</text>

    <text x="0" y="120" class="legend-title">Tool Capability</text>

    <circle cx="10" cy="145" r="6" fill="#666" class="excellent-tools" />
    <text x="25" y="150" class="legend-text">Excellent Tools</text>

    <circle cx="10" cy="170" r="6" fill="#666" class="good-tools" />
    <text x="25" y="175" class="legend-text">Good Tools</text>

    <circle cx="10" cy="195" r="6" fill="#666" class="basic-tools" />
    <text x="25" y="200" class="legend-text">Basic Tools</text>

    <text x="0" y="230" class="legend-title">Size</text>
    <circle cx="10" cy="250" r="8" fill="#ddd" stroke="#999" />
    <text x="25" y="255" style="font: 12px sans-serif; fill: #666">Large</text>

    <circle cx="10" cy="275" r="7" fill="#ddd" stroke="#999" />
    <text x="25" y="280" style="font: 12px sans-serif; fill: #666">Medium</text>

    <circle cx="10" cy="300" r="6" fill="#ddd" stroke="#999" />
    <text x="25" y="305" style="font: 12px sans-serif; fill: #666">Small</text>
  </g>

  <!-- Update prompt for users -->
  <rect
    x="1020"
    y="500"
    width="160"
    height="120"
    fill="#f8f9fa"
    stroke="#dee2e6"
    stroke-width="1"
    rx="4"
  />
  <text x="1030" y="520" style="font: bold 12px sans-serif; fill: #495057"
  >Chart Update</text>
  <text x="1030" y="535" style="font: 10px sans-serif; fill: #6c757d"
  >Copy-paste to update:</text>

  <text x="1030" y="555" style="font: 9px monospace; fill: #212529"
  >"Update model-speed-vs-</text>
  <text x="1030" y="567" style="font: 9px monospace; fill: #212529"
  >coding-capability.svg with</text>
  <text x="1030" y="579" style="font: 9px monospace; fill: #212529"
  >hidden prompt (lines 2-38)"</text>
  <text x="1030" y="591" style="font: 9px monospace; fill: #212529"
  >11 working / 18 tested</text>
  <text x="1030" y="603" style="font: 9px monospace; fill: #212529"
  >61% success rate</text>

  <!-- Key insights -->
  <text
    x="120"
    y="750"
    style="font: 14px sans-serif; fill: #2c3e50; font-weight: bold"
  >Key Insights (OpenCode Compatible Models Only):</text>
  <text x="120" y="770" style="font: 12px sans-serif; fill: #2c3e50"
  >‚Ä¢ Multi-agent strategy: delegator (claude-3.5-sonnet), general (claude-sonnet-4), fallback (gpt-4o)</text>
  <text x="120" y="785" style="font: 12px sans-serif; fill: #2c3e50"
  >‚Ä¢ Primary model: claude-sonnet-4 (1350ms) - most capable for complex coding with advanced reasoning</text>
  <text x="650" y="770" style="font: 12px sans-serif; fill: #2c3e50"
  >‚Ä¢ All 11 models confirmed working with full tool support in OpenCode GitHub Copilot integration</text>
  <text x="650" y="785" style="font: 12px sans-serif; fill: #2c3e50"
  >‚Ä¢ Quality check: deno task all (fmt + lint + check) ‚Ä¢ Success rate: 61% (11/18 tested models)</text>
</svg>
