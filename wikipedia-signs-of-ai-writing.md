# Wikipedia:Signs of AI writing

**This is an advice page from WikiProject AI Cleanup.**

This page is not an encyclopedic article, nor one of Wikipedia's policies or
guidelines, as it has not been thoroughly vetted by the community.

**Shortcuts**

- WP:AISIGNS
- WP:AITELLS
- WP:LLMSIGNS

LLMs tend to have an identifiable writing style.

This is a list of writing and formatting conventions typical of AI chatbots such
as ChatGPT, with real examples taken from Wikipedia articles and drafts. It is
meant to act as a field guide to help detect undisclosed AI-generated content on
Wikipedia. This list is _descriptive_, not _prescriptive_; it consists of
observations, not rules. Advice about formatting or language to avoid in
Wikipedia articles can be found in the policies and guidelines and the Manual of
Style, but does not belong on this page.

This list is _not_ a ban on certain words, phrases, or punctuation. No one is
taking your em-dashes away or claiming that only LLMs use them. Not all text
featuring the following indicators is AI-generated, as the large language models
that power AI chatbots are trained on human writing, including the writing of
Wikipedia editors. This is simply a catalog of very common patterns observed
over many thousands of instances of AI-generated text, _specific to Wikipedia._
While some of its advice may be broadly applicable, some signs—particularly
those involving punctuation and formatting—may not apply in a non-Wikipedia
context.

The patterns here are also only potential _signs_ of a problem, not _the problem
itself_. While many of these issues are immediately obvious and easy to
fix—e.g., excessive boldface, poor wordsmithing, broken markup, citation style
quirks—they can point to less outwardly visible problems that carry much more
serious policy risks. If LLM-generated text is polished enough (initially or
subsequently tidied up), those surface defects might not be present, but the
deeper problems likely will. Please do not merely treat these signs as the
problems to be fixed; that could just make detection harder. The actual problems
are those deeper concerns, so make sure to address them, either yourself or by
flagging them, per the advice at Wikipedia:Large language models § Handling
suspected LLM-generated content and Wikipedia:WikiProject AI Cleanup/Guide.

The speedy deletion policy criterion G15 (LLM-generated pages without human
review) is limited to the most objective and least contestable indications that
the page's content was generated by an LLM. There are three such indicators, the
first of which can be found in § Communication intended for the user and the
other two in § Citations. The other signs, though they may indeed indicate AI
use, are not sufficient for speedy deletion.

Do not solely rely on artificial intelligence content detection tools (such as
GPTZero) to evaluate whether text is LLM-generated. While they perform better
than random chance, these tools have nontrivial error rates and cannot replace
human judgment.

## Contents

- [Language and tone](#language-and-tone)
- [Style](#style)
- [Communication intended for the user](#communication-intended-for-the-user)
- [Markup](#markup)
- [Citations](#citations)
- [Miscellaneous](#miscellaneous)

## Language and tone

**Shortcuts**

- WP:AIWTW
- WP:AI-ISM

LLMs (and artificial neural networks in general) use statistical algorithms to
guess (infer) what should come next based on a large corpus of training
material. It thus tends to regress to the mean; that is, the result tends toward
the most statistically likely result that applies to the widest variety of
cases. It can simultaneously be a strength and a "tell" for detecting
AI-generated content.

For example, LLMs are usually trained on data from the internet in which famous
people are generally described with positive, important-sounding language. It
will thus sand down specific, unusual, nuanced facts (which are statistically
rare) and replace them with more generic, positive descriptions (which are
statistically common). Thus the specific detail "invented a train-coupling
device" might become "a revolutionary titan of industry." LLMs tend to smooth
out unusual details and drift toward the most common, statistically probable way
of describing a topic. It is like shouting louder and louder that a portrait
shows a uniquely important person, while the portrait itself is fading from a
sharp photograph into a blurry, generic sketch. The subject becomes
simultaneously less specific and more exaggerated.

This statistical regression to the mean, a smoothing over of specific facts into
generic statements that could apply to many topics, makes AI-generated content
easier to detect.

### Undue emphasis on symbolism and importance

**Words to watch:** **_stands as / serves as / is a testament_, _plays a
vital/significant/crucial role_, _underscores its importance_, _highlights its
significance_, _continues to captivate_, _leaves a lasting impact_, _watershed
moment_, _key turning point_, _deeply rooted_, _profound heritage_, _steadfast
dedication_, _solidifies_ ...**

LLM writing often puffs up the importance of the subject matter with reminders
that it represents or contributes to a broader topic. There seems to be only a
small repertoire of ways that it writes these reminders, so if they are
otherwise appropriate it would be best to reword them anyway.

When talking about biology (e.g. when asked to discuss a given animal or plant
species), LLMs tend to put too much emphasis on the species' conservation status
and the efforts to protect it, even if the status is unknown and no serious
efforts exist.

### Promotional language

**Words to watch:** **_rich/vibrant cultural heritage/tapestry_, _breathtaking_,
_must-visit/see_, _stunning natural beauty_, _enduring/lasting legacy_,
_nestled_, _in the heart of_ ...**

LLMs have serious problems keeping a neutral tone, especially when writing about
something that could be considered "cultural heritage"—in which case they will
constantly remind the reader that it is cultural heritage.

### Editorializing

**Words to watch:** **_it's important to note/remember/consider_, _it is worth_,
_no discussion would be complete without_, _this article wouldn't exist without_
...**

LLMs often introduce their own interpretation, analysis, and opinions in their
writing, even when they are asked to write neutrally, violating the policy No
original research. Editorializing can appear through specific words or phrases
or within broader sentence structures. This indicator often overlaps with other
language and tone indicators in this list. Note that humans and especially new
editors often make this mistake as well.

### Overuse of certain conjunctions

While human writing obviously contains connecting words and phrases, LLMs tend
to _overuse_ them, in a stilted, formulaic way. This is often a byproduct of an
essay-like structure that implies synthesis of facts, which is typical of LLM
writing but inappropriate for Wikipedia.

### Section summaries

**Shortcuts**

- WP:CONCLUSION
- WP:INCONCLUSION

**Words to watch:** **_In summary_, _In conclusion_, _Overall_ ...**

LLMs will often end a paragraph or section by summarizing and restating its core
idea. While this may be permitted for some scholarly writing, proper Wikipedia
writing typically never summarizes the general idea of a block of article text
(besides the lead section being a summary of the entire article).

### Outline-like conclusions about challenges and future prospects

**Words to watch:** **_Despite its... faces several challenges..._, _Despite
these challenges_, _Challenges and Legacy_, _Future Outlook_ ...**

Many LLM-generated Wikipedia articles include a "Challenges" section, which
typically begins with a sentence like "Despite its [positive/promotional words],
[article subject] faces challenges..." and ends with either a positive
assessment of the article subject, or speculation about how ongoing or potential
initiatives could benefit the subject. Such paragraphs usually appear at the end
of articles with a rigid outline structure, which may also include a separate
section for "Future Prospects."

Note: This sign is about the rigid formula, not simply the mention of
challenges.

### Negative parallelisms

**Shortcut**

- WP:AIPARALLEL

Parallel constructions involving "not", "but", or "however" such as "Not only
... but ..." or "It is not just about ..., it's ..." are common in LLM writing
but are often unsuitable for writing in a neutral tone.

### Rule of three

LLMs overuse the 'rule of three'—"the good, the bad, and the ugly". This can
take different forms from "adjective, adjective, adjective" to "short phrase,
short phrase, and short phrase". LLMs often use this structure to make
superficial analyses appear more comprehensive.

### Superficial analyses

**Words to watch:** **_ensuring ..._, _highlighting ..._, _emphasizing ..._,
_reflecting ..._, _underscoring ..._**

AI chatbots tend to insert superficial analysis of information, often in
relation to its significance, recognition, or impact. This is often done by
attaching a present participle ("-ing") phrase at the end of sentences,
sometimes with vague attributions to third parties (see below). These comments
are generally unhelpful as they introduce unnecessary or fictional opinions.

### Vague attributions of opinion

**Shortcut**

- WP:AIWEASEL

**Words to watch:** **_Industry reports_, _Observers have cited_, _Some critics
argue_ ...**

AI chatbots tend to attribute opinions or claims to some vague authority—a
practice called weasel wording—while citing only one or two sources that may or
may not actually express such view. They also tend to overgeneralize a
perspective of one or few sources into that of a wider group.

### Noun Over-Variation

Generative AI has a repetition-penalty code, such that it is not allowed to
repeat words too often. In the case of adjectives and verbs, this often helps.
The problem arises with nouns, particularly the entry's main subject, when the
guide to vary causes confusion. For instance, the entry might give a main
character's name and then repeatedly replace it with co-referring nouns (e.g.,
protagonist, key player, eponymous character). This reduces the unity of the
article and causes cognitive load for the reader, who must mentally reestablish
that the same thing is being talked about.

### False range

**Shortcuts**

- WP:FROMXTOY
- WP:FALSERANGE

When giving examples of items within a set, AI chatbots will often mention these
items within a phrase that reads "from ... to ...", which often results in a
non-encyclopedic tone. This indicator is not to be confused with the
prepositions' non-figurative usage, such as in spatial or temporal contexts
(e.g. "... went from Chicago to Los Angeles", "... the library will be closed
from Friday to Wednesday").

## Style

### Title case in section headings

**Shortcut**

- WP:AITITLECASE

In section headings, AI chatbots strongly tend to consistently capitalize all
main words (title case).

### Excessive use of boldface

**Shortcut**

- WP:AIBOLD

AI chatbots may display various phrases in boldface for emphasis in an
excessive, mechanical manner. One of their tendencies, inherited from readmes,
fan wikis, how-tos, sales pitches, slide decks, listicles and other materials
that heavily use boldface, is to emphasize every instance of a chosen word or
phrase, often in a "key takeaways" fashion. Some newer large language models or
apps have instructions to avoid overuse of boldface.

### Lists

**Shortcut**

- WP:AILIST

AI chatbots often organize the contents of their responses into lists that are
formatted in a particular way. A very common example is a "bullet points with
bold titles" style, in which the content of each bullet point is a longer
rewording of the bolded keyword preceding it.

Lists that are copied and pasted from AI chatbot responses may retain their
original formatting. Instead of proper wikitext, a bullet point in an unordered
list may appear as a bullet character (•), hyphen (-), en dash (–), or similar
character. Ordered lists (i.e. numbered lists) may use explicit numbers (such as
`1.`) instead of standard wikitext.

### Emoji

Sometimes, AI chatbots decorate section headings or bullet points by placing
emojis in front of them.

### Overuse of em dashes

**Shortcut**

- WP:AIDASH

While human editors may use em dashes (—), LLM output tends to use them more
often than human-written text of the same genre, and uses them in places where
humans are more likely to use commas, parentheses, colons, or (misused) hyphens
(-). LLMs especially tend to use em-dashes in a formulaic, pat way, often
mimicking "punching up" sales-like writing by over-emphasizing clauses or
parallelisms.

This sign is most useful when taken in combination with other indicators, not by
itself.

### Curly quotation marks and apostrophes

**Shortcut**

- WP:AICURLY

AI chatbots typically use curly quotation marks ("..." or '...') instead of
straight quotation marks ("..." or '...'). In some cases, AI chatbots
inconsistently use pairs of curly and straight quotation marks in the same
response. They also tend to use the curly apostrophe ('; the same character as
the curly right single quotation mark) instead of the straight apostrophe ('),
such as in contractions and possessive forms. They may also do this
inconsistently.

Curly quotes alone do not prove LLM use. Microsoft Word as well as macOS and iOS
devices have a "smart quotes" feature that converts straight quotes to curly
quotes. Grammar correcting tools such as LanguageTool may also have such a
feature. Curly quotation marks and apostrophes are common in professionally
typeset works such as major newspapers.

### Letter-like writing

**Shortcut**

- WP:AILETTER

**Words to watch:** **_Subject:_, _Dear Wikipedia Editors/Administrators_, _I
hope this message finds you well_, _I am writing to..._, _I am willing/would be
happy to..._, _Thank you for your time/consideration..._, ...**

Talk page messages and unblock requests generated by AI chatbots often include
salutations and valedictions. Many messages emphasize a user's good faith and
promise that the user will adhere to Wikipedia's guidelines. The presence of a
subject line above the text, intended to fill the Subject line on an email form,
is a more definitive tell.

## Communication intended for the user

### Collaborative communication

**Shortcuts**

- WP:CERTAINLY
- WP:AITALKINARTICLE
- WP:COLLABCOMM

**Words to watch:** **_I hope this helps_, _Of course!_, _Certainly!_, _You're
absolutely right!_, _Would you like..._, _is there anything else_, _let me
know_, _more detailed breakdown_, _here is a_ ...**

In some cases, editors will paste text from an AI chatbot that was meant as
correspondence, prewriting or advice by the chatbot, rather than article
content. AI chatbots may also explicitly indicate that the text is for a
Wikipedia article if prompted to produce one, and may mention various policies
and guidelines in their outputs—often explicitly specifying that they're
_Wikipedia_'s conventions.

### Knowledge-cutoff disclaimers and speculation about gaps in sources

**Shortcuts**

- WP:AICUTOFF
- WP:AIDISCLAIMER

**Words to watch:** **_as of [date]_,** _Up to my last training update_, _as of
my last knowledge update_, _While specific details are limited/scarce..._, _not
widely available/documented/disclosed_, _...in the provided/available
sources/search results..._, _based on available information_ ...**

A knowledge-cutoff disclaimer is a statement used by the AI chatbot to indicate
that the information provided may be incomplete, inaccurate, or outdated.

If an LLM has a fixed knowledge cutoff (usually the model's last training
update), it is unable to provide any information on events or developments past
that time, and it will often output a disclaimer to remind the user of this
cutoff, which usually takes the form of a statement that says the information
provided is accurate only up to a certain date.

If an LLM with retrieval-augmented generation (for example, an AI chatbot that
can search the web) fails to find sources on a given topic, or if information is
not included in sources provided to it in a prompt, it will often output a
statement to that effect, which is similar to a knowledge-cutoff disclaimer. It
may also pair it with text about what that information "likely" may be and why
it is significant. This information is entirely speculative (including the very
claim that it's "not documented") and may be based on loosely related topics or
completely fabricated. It is also frequently combined with the tells above.

### Prompt refusal

**Words to watch:** **_as an AI language model_, _as a large language model_,
_I'm sorry_ ...**

Occasionally, the AI chatbot will decline to answer a prompt as written, usually
with an apology and a reminder that it is "an AI language model". Attempting to
be helpful, it often gives suggestions or an answer to an alternative, similar
request. Outright refusals have become increasingly rare.

Prompt refusals are obviously unacceptable for Wikipedia articles, but some
users do include them. This may indicate that the user did not review the text,
or that they may not have a proficient grasp on the English language. Remember
to assume good faith, because the editor may genuinely want to improve our
coverage of knowledge gaps.

### Phrasal templates and placeholder text

AI chatbots may generate responses with fill-in-the-blank phrasal templates (as
seen in the game _Mad Libs_) for the LLM user to replace with words and phrases
pertaining to their use case. However, some LLM users forget to add such words.
Note that non-LLM-generated templates exist for drafts and new articles, such as
Wikipedia:Artist biography article template/Preload and pages in
Category:Article creation templates.

## Markup

### Use of Markdown

**Shortcut**

- WP:MARKDOWN

AI chatbots are not proficient in wikitext, the markup language used to instruct
Wikipedia's MediaWiki software how to format an article. As wikitext is mostly
tied to a specific platform using a specific software (a wiki running on
MediaWiki), it is a niche markup language, lacking wider exposure beyond
Wikipedia and other MediaWiki-based platforms like Miraheze. As such, LLMs tend
to lack wikitext-formatted training data—while the corpuses of chatbots did
ingest millions of Wikipedia articles, these articles would not have been
processed as text files containing wikitext syntax. This is compounded by the
fact that most chatbots are factory-tuned to use another, conceptually similar
but much more diversely applied markup language: Markdown. Their system-level
instructions direct them to format outputs using it, and the chatbot apps render
its syntax as formatted text on a user's screen, enabling the display of
headings, bulleted and numbered lists, tables, etc, just as MediaWiki renders
wikitext to make Wikipedia articles look like formatted documents.

The presence of faulty wikitext syntax mixed with Markdown syntax is a strong
indicator that content is LLM-generated, especially if in the form of a fenced
Markdown code block. However, Markdown _alone_ is not such a strong indicator.
Software developers, researchers, technical writers, and experienced internet
users frequently use Markdown in tools like Obsidian and GitHub, and on
platforms like Reddit, Discord, and Slack. Some writing tools and apps, such as
iOS Notes, Google Docs, and Windows Notepad, may support Markdown editing or
exporting. The increasing ubiquity of Markdown may also lead new editors to
expect or assume Wikipedia to support Markdown by default.

### Broken wikitext

As explained above, AI-chatbots are not proficient in wikitext and Wikipedia
templates, leading to faulty syntax. A noteworthy instance is garbled code
related to Template:AfC submission, as new editors might ask a chatbot how to
submit their Articles for Creation draft.

### turn0search0

ChatGPT may include `citeturn0search0` (surrounded by Unicode points in the
Private Use Area) at the ends of sentences, with the "search" number increasing
as the text progresses. These are places where the chatbot links to an external
site, but a human pasting the conversation into Wikipedia has that link
converted into placeholder code. This was first observed in February 2025.

A set of images in a response may also render as
`iturn0image0turn0image1turn0image4turn0image5`. Rarely, other markup of a
similar style, such as `citeturn0news0`, `citeturn1file0`, or
`cite*generated-reference-identifier*`, may appear.

### contentReference, oaicite, and oai_citation

**Shortcut**

- WP:OAICITE

Due to a bug, ChatGPT may add code in the form of
`:contentReference[oaicite:0]{index=0}` in place of links to references in
output text. Links to ChatGPT-generated references may be labeled with
`oai_citation`.

### attribution and attributableIndex

ChatGPT may add JSON-formatted code at the end of sentences in the form of
`({"attribution":{"attributableIndex":"X-Y"}})`, with X and Y being increasing
numeric indices.

### utm_source=

ChatGPT may add the URL search parameter `utm_source=chatgpt.com` or, as of
August 2025, `utm_source=openai` to URLs that it is using as sources.

### Named references declared in references section but unused in article body

AI chatbots may generate reference sections with named references that are never
actually used in the article body text.

### Non-existent categories

**Shortcut**

- WP:AIREDCAT

LLMs sometimes hallucinate non-existent categories (which appear as red links)
because their training set includes obsolete and renamed categories that they
reproduce in new content. They may also treat ordinary references to topics as
categories, thus generating non-existent categories. Note that this is also a
common error made by new or returning editors.

## Citations

**Shortcut**

- WP:AIFICTREF

### Broken external links

If a new article or draft has multiple citations with external links, and most
of them are broken (error 404 pages), this is a clear sign of an AI-generated
page, particularly if the dead links are not found in website archiving sites
like Internet Archive or Archive Today. Most links become broken (see link rot)
over time, but those factors make it unlikely that the link was ever valid.

### Invalid DOI and ISBNs

A checksum can be used to verify ISBNs. An invalid checksum is a very likely
sign that an ISBN is incorrect, and citation templates will display a warning if
so. Similarly, DOIs are more resistant to link rot than regular hyperlinks.
Unresolvable DOIs and invalid ISBNs can be indicators of hallucinated
references.

### Incorrect or unconventional use of references

**Shortcut**

- WP:AICITESTYLE

AI tools may have been prompted to include references, and make an attempt to do
so as Wikipedia expects, but fail with some key implementation details or stand
out when compared with conventions.

## Miscellaneous

### Abrupt cut offs

AI tools may abruptly stop generating content, for example if they predict the
end of text sequence (appearing as <|endoftext|>) next. Also, the number of
tokens that a single response has is usually limited and further responses will
require the user to select "continue generating".

This method is not foolproof, as a malformed copy/paste from one's local
computer can also cause this. It may also indicate a copyright violation rather
than the use of an LLM.

### Discrepancies in writing style and variety of English

A sudden shift in an editor's writing style, such as unexpectedly flawless
grammar compared to their other communication, may indicate the use of AI tools.

Another discrepancy is a mismatch of user location, national ties of the topic
to a variety of English, and the variety of English used. A human writer from
India writing about an Indian university would probably not use American
English; however, LLM outputs use American English by default, unless prompted
otherwise. Note that non-native English speakers tend to mix up English
varieties, and such signs should only raise suspicion if there is a sudden and
complete shift in an editor's English variety use.

### Age of text relative to ChatGPT launch

ChatGPT was launched to the public on November 30, 2022. Although OpenAI had
similarly powerful LLMs before then, they were paid services and not
particularly accessible or known to lay people. ChatGPT experienced extreme
growth immediately on launch.

It is very unlikely that any particular text added to Wikipedia **prior to
November 30, 2022** was generated by an LLM. If an edit to a page was made
before this date, AI use can be safely ruled out for that revision. While some
text added as far back as 20 years ago may appear to match some of the AI signs
given in this list, and even convincingly appear to have been AI generated, the
vastness of Wikipedia allows for these rare coincidences.

### Overwhelmingly verbose edit summaries

AI-generated edit summaries are often unusually long, written as formal,
first-person paragraphs without abbreviations, and/or conspicuously itemize
Wikipedia's conventions.

Most editors using AI do not ask for summaries to be generated.

## See also

- Wikipedia:Artificial intelligence

---

_Retrieved from the Wikipedia page "Wikipedia:Signs of AI writing" on October 1,
2025_
